<!doctype html><html lang=en-us><head><title>BrainHack 2020</title><meta name=twitter:card content="summary"><meta name=twitter:site content="@brainhack"><meta name=twitter:title content="Load fMRIprep confounds in Python"><meta name=twitter:description content="load_confounds is a tool for loading a sensible subset of the fMRI confounds generated with fMRIprep in python (Esteban et al., 2019). The outputs can be directly passes to nilearn NifitMasker for denoising. The aim at Brainhack MTL 2020 is to implement new strategies as well as imporving the existing functions and documentations for a potential Beta release."><meta name=twitter:image content="https://github.com/htwangtw/2021_08_Wang_load-confounds/blob/gh-pages/figures.css?raw=true"><meta name=apple-mobile-web-app-capable content="yes"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no,user-scalable=no"><meta name=description content="Proceeding page."><meta name=author content="OHBM Brainhack 2020"><link rel="shortcut icon" href=favicon.svg><link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap" rel=stylesheet><link rel=stylesheet href=//brainhack-proceedings.github.io/template/assets/vendor/bootstrap/css/bootstrap.min.css><link rel=stylesheet href=//brainhack-proceedings.github.io/template/assets/css/business-frontpage.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css><link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900|Material+Icons" rel=stylesheet type=text/css><link href="https://fonts.googleapis.com/css2?family=Roboto+Condensed:wght@300;700&display=swap" rel=stylesheet><link href=https://use.fontawesome.com/releases/v5.0.13/css/all.css rel=stylesheet type=text/css><link href=https://cdn.jsdelivr.net/npm/quasar@1.12.0/dist/quasar.min.css rel=stylesheet type=text/css><link href=https://cdn.jsdelivr.net/npm/@quasar/quasar-ui-qflashcard@1.0.1/dist/index.min.css rel=stylesheet type=text/css><style>code{padding:2px 4px;font-size:90%;color:#5285b6;background-color:#e7e7e7;border-radius:4px}html,body,h1,h2,h3,h4,h5,h6{font-family:roboto,cursive,sans-serif;-webkit-font-smoothing:antialiased;color:#2e2e2e}html,body{min-height:100vh;scrollbar-width:thin}body::-webkit-scrollbar{width:5px}*::-webkit-scrollbar-track{background:#ccc}*::-webkit-scrollbar-thumb{background-color:#424344;border-left:1px solid #ccca}body{width:100%;display:flex;flex-direction:column}header{background-color:#424344!important;background-repeat:no-repeat!important;background-position:50%!important}h1{font-size:2rem;font-weight:300;color:#3b515e}h2{font-size:1.5rem}h3{font-size:1rem;font-weight:300}p{font-size:1rem}.btn{background-color:#3b515e;border:none;color:#fff;cursor:pointer;margin-top:.5em;padding:.5rem;font-size:1rem;width:100%}.btn:hover{box-shadow:3px 3px 3px rgba(0,0,0,.4)!important;background-color:#424344;color:#fff}.btn .fa{margin-right:.7rem}.logo{}.logo-brainhack{font-weight:900;font-family:roboto condensed,sans-serif}.logo-proceedings{font-weight:300}</style><script src=//brainhack-proceedings.github.io/template/assets/vendor/jquery/jquery.min.js></script><script src=//brainhack-proceedings.github.io/template/assets/vendor/bootstrap/js/bootstrap.bundle.min.js></script><script>$(function(){$('[data-toggle="tooltip"]').tooltip()})</script><style>main img{max-width:95%;height:auto}main h1.title{font-weight:400!important;margin-top:5rem;margin-bottom:.5em!important;line-height:1em;margin-right:1rem!important}h2{font-weight:400;margin-top:1em}h3{font-weight:400;line-height:2;color:#333;margin:0;padding:0;text-transform:uppercase;margin-top:1em}[v-cloak]{display:none}#content{flex-grow:1}main p{text-align:justify;hyphens:auto}main{margin-left:4rem;margin-right:1rem}.tags{list-style:none;margin:0;overflow:hidden;padding:0}.tags li{float:left}.tag{background:#3b515e;border-radius:3px 0 0 3px;color:#f0f8ff;display:inline-block;height:26px;line-height:26px;padding:0 20px 0 23px;position:relative;margin:0 10px 10px 0;text-decoration:none;-webkit-transition:color .2s}.tag::before{background:#fff;border-radius:10px;box-shadow:inset 0 1px rgba(0,0,0,.25);content:'';height:6px;left:10px;position:absolute;width:6px;top:10px}.tag::after{background:#fff;border-bottom:13px solid transparent;border-left:10px solid #3b515e;border-top:13px solid transparent;content:'';position:absolute;right:0;top:0}.tag:hover{background-color:#424344;color:#fff}.tag:hover::after{border-left-color:#424344}.comma-list{display:inline;list-style:none;padding:0;font-family:open sans,cursive,sans-serif;-webkit-font-smoothing:antialiased;font-size:large}.comma-list li{display:inline}.comma-list li::after{content:", "}.comma-list li:last-child::after{content:""}.tooltip .arrow:before{border-bottom-color:#000!important;border-top-color:#000!important}.tooltip-inner{background-color:#3b515e}.color-on-hover{filter:saturate(0)}.color-on-hover:hover{filter:saturate(1)}.right-panel{width:222px;color:#888;line-height:1.3rem;margin-top:5rem;margin-left:5rem}.right-panel p{font-size:14px}ul.suppl{padding-left:2rem}ul.affiliations{font-size:.9rem!important;margin-bottom:5px!important}ul.affiliations li{list-style:none}ul.affiliations li span{vertical-align:super;font-size:.7rem!important}ul.authors{display:block;font-size:1.2rem!important;margin-bottom:5px!important}ul.authors::after{content:"";clear:both;display:block}ul.authors li{list-style:none}ul.authors li span{vertical-align:super;font-size:.7rem!important;margin-left:-4px}</style></head><style>nav{background-color:#424344!important}nav>.container{max-width:100%}.navbar-nav{font-size:14px}.nav-item{transition:background .3s;padding-top:1rem;padding-left:40px;padding-right:40px;height:4rem;margin-bottom:-2rem}.nav-item:hover{background:rgba(255,255,255,.1)}.nav-item a{text-decoration:none;color:#fff}nav .logo{font-size:31px}nav .logo-brainhack{margin-left:30px}</style><nav class="navbar navbar-expand-lg navbar-dark fixed-top"><div class=container><a class=navbar-brand href=https://ohbm.github.io/hackathon2020/><span class="logo logo-brainhack">BRAINHACK</span><span class="logo logo-proceedings">Proceedings</span></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a href=https://ohbm.github.io/hackathon2020/>About</a></li><li class=nav-item><a href=https://brainhack-proceedings.github.io/papers>Papers</a></li><li class=nav-item><a href=https://brainhack-proceedings.github.io/blog>Blog</a></li><li class=nav-item><a href=https://github.com/brainhack-proceedings/submit>Submit</a></li></ul></div></div></nav><body><div id=content class=container-fluid><div class=row><div class=right-panel><div class=color-on-hover><style>#share-buttons{vertical-align:middle}#share-buttons:after{content:"";display:block;clear:both}#share-buttons>div{position:relative;text-align:left;height:36px;width:32px;float:left;text-align:left}#share-buttons>div>svg{height:16px;fill:#3b515e;margin-top:5px}#share-buttons>div:hover{cursor:pointer}#share-buttons>div.twitter:hover>svg{fill:#7d7d7d}#share-buttons>div.linkedin:hover>svg{fill:#7d7d7d}#share-buttons>div.mail:hover>svg{fill:#7d7d7d}#share-buttons>div.twitter>svg{height:20px;margin-top:8px}#share-buttons>div.linkedin>svg{height:19px;margin-top:7px}#share-buttons>div.mail>svg{height:14px;margin-top:11px}</style><div id=share-buttons><div class=twitter title="Share this on Twitter" onclick="window.open('https://twitter.com/intent/tweet?url=https:\/\/htwangtw.github.io\/2021_08_Wang_load-confounds');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5T1369.5 1125 1185 1335.5t-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5T285 1033q33 5 61 5 43 0 85-11-112-23-185.5-111.5T172 710v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5T884 653q-8-38-8-74 0-134 94.5-228.5T1199 256q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div><div class=linkedin title="Share this on Linkedin" onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=https:\/\/htwangtw.github.io\/2021_08_Wang_load-confounds&title=&summary=&source=');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M477 625v991H147V625h330zm21-306q1 73-50.5 122T312 490h-2q-82 0-132-49t-50-122q0-74 51.5-122.5T314 148t133 48.5T498 319zm1166 729v568h-329v-530q0-105-40.5-164.5T1168 862q-63 0-105.5 34.5T999 982q-11 30-11 81v553H659q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5T1285 602q171 0 275 113.5t104 332.5z"/></svg></div><div class=mail title="Share this through Email" onclick="window.open('mailto:?&body=https:\/\/htwangtw.github.io\/2021_08_Wang_load-confounds');"><svg viewBox="0 0 1792 1792" xmlns="http://www.w3.org/2000/svg"><path d="M1792 710v794q0 66-47 113t-113 47H160q-66 0-113-47T0 1504V710q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 1e2-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38T639 1015q-91-64-262-182.5T172 690q-62-42-117-115.5T0 438q0-78 41.5-130T160 256h1472q65 0 112.5 47t47.5 113z"/></svg></div></div><a href=https://github.com/htwangtw/2021_08_Wang_load-confounds/actions target=blank><img alt style=display:inline-block;height:20px!important src="https://github.com/htwangtw/2021_08_Wang_load-confounds/workflows/Report/badge.svg?branch=main"></a>
<a href=https://github.com/SIMEXP/load_confounds target=_blank style=text-decoration-line:none><img alt style=display:inline-block;height:20px!important src="https://img.shields.io/badge/Project-Repo-red.svg?logo=github&logoColor=&logoWidth=14&style="></a>
<a href=https://github.com/htwangtw/2021_08_Wang_load-confounds/raw/gh-pages/pdf/brainhack-2021_08_Wang_load-confounds.pdf download=brainhack-2021_08_Wang_load-confounds.pdf><button class=btn>
<i class="fa fa-download"></i>Download PDF</button></a></div><h3>Contributions</h3><p>HTW, SLM wrote the software, HTW, SLM wrote the report</p><h3>Competing interests</h3><p>None</p><h3>Acknowledgements</h3><p>The authors would like to thank the organizers and attendees of Brainhack Global Montreal 2020. Especially, we would like to thank Elizabeth DuPre and Chris Markiewicz for discussions on the choice of test data.</p><h3>Event</h3><p>Brainhack Global 2020 Montreal</p><h3>License</h3><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Cc.logo.circle.svg/1280px-Cc.logo.circle.svg.png width=30px>
<a href=https://creativecommons.org/licenses/by/4.0/ target=_blank>Creative Commons Attribution license (CC BY 4.0)</a></div><main class="col-xs-12 col-sm-7 col-pull-7"><h1 class=title>Load fMRIprep confounds in Python</h1><ul class="comma-list authors"><li data-toggle=tooltip title="CRIUGM, Montreal, Canada">Pierre Bellec
<span>1</span></li><li data-toggle=tooltip title="CRIUGM, Montreal, Canada">Hao-Ting Wang
<span>1</span></li><li data-toggle=tooltip title="Harvard University, Cambridge, USA">Steven Lee Meisler
<span>2</span></li></ul><ul class="comma-list affiliations"><li data-toggle=tooltip title="CRIUGM, Montreal, Canada"><span>1</span>CRIUGM</li><li data-toggle=tooltip title="Harvard University, Cambridge, USA"><span>2</span>Harvard University</li></ul><h2>Abstract</h2><p>load_confounds is a tool for loading a sensible subset of the fMRI confounds generated with fMRIprep in python (Esteban et al., 2019). The outputs can be directly passes to nilearn NifitMasker for denoising. The aim at Brainhack MTL 2020 is to implement new strategies as well as imporving the existing functions and documentations for a potential Beta release.</p><ul class=tags><li class=tag>fmriprep</li><li class=tag>confounds</li><li class=tag>nilearn</li></ul><!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang><head><meta charset=utf-8><meta name=generator content="pandoc"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><title>brainhack-2021_08_Wang_load-confounds-html</title><style>code{white-space:pre-wrap}span.smallcaps{font-variant:small-caps}span.underline{text-decoration:underline}div.column{display:inline-block;vertical-align:top;width:50%}div.hanging-indent{margin-left:1.5em;text-indent:-1.5em}ul.task-list{list-style:none}</style><link rel=stylesheet href=./css/figures.css><script type=text/x-mathjax-config>
    MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
  </script></head><body><h1 id=introduction>Introduction</h1><p>fMRIprep <span class=citation data-cites=fmriprep:2019>(Esteban et al. 2019)</span> is a popular minimal preprocessing software for functional MRI data. ‘Minimal preprocessing’ refers to motion correction, field unwarping, normalization, bias field correction, and brain extraction. Confound regression and smoothing are excluded from the workflow. Instead, fMRIprep provides users with a large set of potential confound regressors that covers many denoising strategies. The users have to select the confound regressors for denoising in subsequent analyses. Loading a sensible subset of confounds is difficult and error-prone for many strategies, such as ICA-AROMA <span class=citation data-cites=icaaroma:2015>(Pruim et al. 2015)</span> and CompCor <span class=citation data-cites=compcor:2007>(Behzadi et al. 2007)</span>. <span style=background-color:light-gray><code>load confounds</code></span> can extract confound variables and implement preset strategies for confound selections. The loaded format is compatible with <span style=background-color:light-gray><code>nilearn</code></span> analysis functions such as <span style=background-color:light-gray><code>NiftiMasker</code></span> and the GLM modules. Our aim was to provide a easy and foolproof API for users to perform subsequent denoising of <span style=background-color:light-gray><code>fMRIprep</code></span> outputs.</p><h1 id=progress>Progress</h1><p>At Brianhack Global Montreal 2020, our goal was to prepare the package for a potential Beta release. To prepare for the release, related issues involved implementing several preset strategies and improving the user experience with better examples and error messages. Several issues had been identified before Brainhack, and the full discussion can be found under the <span style=background-color:light-gray><code>load confounds</code></span> <a href="https://github.com/SIMEXP/load_confounds/issues?q=is%3Aissue+label%3AbrainhackMTL2020+is%3Aclosed">GitHub issue page</a>. Participants at Brainhack were encouraged to pick up existing issues from the list. The following issues have been discussed and/or resolved:</p><h2 id=strategies>Strategies</h2><p>We worked on three strategies:</p><ul><li><p>Added ICA-AROMA <span class=citation data-cites=icaaroma:2015>(Pruim et al. 2015)</span> (contributed by Hao-Ting Wang)</p></li><li><p>Added Scrubbing <span class=citation data-cites=power:2012>(Power et al. 2012)</span>(contributed by Steven Meisler)</p></li><li><p>Improved the anatomical mask selection for anatomical CompCor <span class=citation data-cites=compcor:2007>(Behzadi et al. 2007)</span> (contributed by Steven Meisler)</p></li></ul><h3 id=ica-aroma>ICA-AROMA</h3><p>ICA-AROMA is only applicable to <span style=background-color:light-gray><code>fMRIprep</code></span> outputs generated with <span style=background-color:light-gray><code>usearoma</code></span>. <span style=background-color:light-gray><code>fMRIprep</code></span> produces files with suffix <span style=background-color:light-gray><code>descsmoothAROMAnonaggr bold</code></span> and outputs ICA components in the confounds file. Using the <span style=background-color:light-gray><code>descsmoothAROMAnonaggr bold</code></span> output is the recommanded way of applying ICA-AROMA and implemented in <span style=background-color:light-gray><code>load confounds</code></span> as a preset strategy. When passing regular fMRIprep outputs suffixed with <span style=background-color:light-gray><code>descprepro bold</code></span>, <span style=background-color:light-gray><code>load confounds</code></span> retreives the noise independent compoenents for aggressive denoising.</p><h3 id=scrubbing>Scrubbing</h3><p><span style=background-color:light-gray><code>load confounds</code></span> provides both a basic and full approach for scrubbing. The basic scrubbing approach flags and censors time frames with excessive motion, using thresholds on framewise displacement and standardized DVARS. For full scrubbing, described in Power et al <span class=citation data-cites=power:2012>(Power et al. 2012)</span>, after censoring volumes as in the basic approach, the full approach further remove continuous segments containing fewer than 5 volumes.</p><h3 id=anatomical-compcor>Anatomical CompCor</h3><p><span style=background-color:light-gray><code>fMRIprep</code></span> uses three kinds of anatomical masks to compute CompCor components, WM, CSF and the combination of the two. Metadata in fMRIPrep’s confounds .json file specify the anatomical compartment associated with each component. The previous iteration of <span style=background-color:light-gray><code>load confounds</code></span> did not consider the metadata, hence the difference of masks were not taken into account. In the revised aCompCor approach, <span style=background-color:light-gray><code>load confounds</code></span> provides selections of anaotomical maps (WM, CSF, and combined) to avoid regressing relevant signal twice, which may introduce noise.</p><h2 id=demo>Demo</h2><p>An executable demo using the nilearn developmental fMRI dataset (<a href=https://openneuro.org/datasets/ds000228>OpenNeuro ds000228</a>) was added (contributed by Michael W. Weiss). The demo was adapted from an exisitng nilearn example on denoising <a href=https://nilearn.github.io/auto_examples/03_connectivity/plot_signal_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-signal-extraction-py>’Extracting signals from a brain parcellation’</a>. This example notebook shows how to extract signals from a brain parcellation and compute a correlation matrix, using different denoising strategies from the package.</p><h2 id=error-message>Error message</h2><p>A new class <span style=background-color:light-gray><code>NotFoundConfoundException</code></span> was added for collecting all the missing parameters needed for the given noise component(s). A final error would be raised with the list of all parameters missing, rather than just the first encountered missing parameter. (Contributed by Michael W. Weiss and François Paugam)</p><h2 id=identify-test-dataset>Identify test dataset</h2><p><a href=https://openneuro.org/datasets/ds000003>OpenNeuro ds000003</a> was adopted as the new test data, including all ICA-AROMA related components. However, non-steady-state volume and motion estmation mertic RMSD <span class=citation data-cites=jenkinson:2002>(Jenkinson et al. 2002)</span> are still missing. We plan on preprocessing the nilearn demo dataset (<a href=https://openneuro.org/datasets/ds000228>OpenNeuro ds000228</a>) with the latest stable fMRIprep LTS release for collecting all possible confounds in the future. (Discussions amongs Pierre Bellec, Hao-Ting Wang, Elizabeth DuPre, and Chris Markiewicz)</p><h2 id=all-contributor-bot>All contributor bot</h2><p><span style=background-color:light-gray><code>allcontributors</code></span> bot is added to track community contributions (Contributed by Pierre Bellec).</p><h2 id=add-to>Add <span style=background-color:light-gray><code>load confounds</code></span> to <span style=background-color:light-gray><code>nixtract</code></span></h2><p>In addition to the main package, there was a collaborative project with the developers of <a href=https://github.com/danjgale/nixtract><span style=background-color:light-gray><code>nixtract</code></span></a>. <span style=background-color:light-gray><code>nixtract</code></span> is a tool that extract and process timeseries data from neuroimaging files. Annabelle Harvey and Dan Gale added <span style=background-color:light-gray><code>load confounds</code></span> as a dependency of <span style=background-color:light-gray><code>nixtract</code></span> for reading fMRIPrep confound variables.</p><h1 id=results>Results</h1><p><span style=background-color:light-gray><code>load confounds</code></span> has now covered most of the strategies tested in Ciric et al. <span class=citation data-cites=ciric:2017>(Ciric et al. 2017)</span>. The following noise components and a set of paramaters for dedicated approaches are supported.</p><ul><li><p><span style=background-color:light-gray><code>motion</code></span>: motion parameters, including 6 translation/rotation (<span style=background-color:light-gray><code>basic</code></span>), and optionally derivatives, squares, and squared derivatives (<span style=background-color:light-gray><code>full</code></span>).</p></li><li><p><span style=background-color:light-gray><code>high pass</code></span>: basis of discrete cosines covering slow time drift frequency band.</p></li><li><p><span style=background-color:light-gray><code>wm csf</code></span> the average signal of white matter and cerebrospinal fluid masks (<span style=background-color:light-gray><code>basic</code></span>), and optionally derivatives, squares, and squared derivatives (<span style=background-color:light-gray><code>full</code></span>).</p></li><li><p><span style=background-color:light-gray><code>global</code></span>: the global signal (<span style=background-color:light-gray><code>basic</code></span>), and optionally derivatives, squares, and squared derivatives (<span style=background-color:light-gray><code>full</code></span>).</p></li><li><p><span style=background-color:light-gray><code>compcor</code></span> <span class=citation data-cites=compcor:2007>(Behzadi et al. 2007)</span>: the results of a PCA applied on a mask based on either anatomy (<span style=background-color:light-gray><code>anat</code></span>) or temporal variance (<span style=background-color:light-gray><code>temp</code></span>). For aCompCor, one can choose either separate or combined WM and CSF compartments, as well as choosing to extract components contributing 50% of variance or a set number of components (typically 5).</p></li><li><p><span style=background-color:light-gray><code>ica aroma</code></span> <span class=citation data-cites=icaaroma:2015>(Pruim et al. 2015)</span>: the results of an idependent component analysis (ICA) followed by identification of noise components. This can be implementing by incorporating ICA regressors (<span style=background-color:light-gray><code>basic</code></span>) or directly loading a denoised file generated by fMRIprep (<span style=background-color:light-gray><code>full</code></span>).</p></li><li><p><span style=background-color:light-gray><code>scrub</code></span> <span class=citation data-cites=power:2012>(Power et al. 2012)</span>: regressors coding for time frames with excessive motion, using thresholds on framewise displacement and standardized DVARS (<span style=background-color:light-gray><code>basic</code></span>) and suppressing short time windows using the (Power et al., 2014) appreach (<span style=background-color:light-gray><code>full</code></span>).</p></li></ul><p><span style=background-color:light-gray><code>load confounds</code></span> can produce the following strategies from Ciric et al. <span class=citation data-cites=ciric:2017>(Ciric et al. 2017)</span>. The following table highlights the relevant options:</p><table><thead><tr class=header><th style=text-align:left>Strategy</th><th style=text-align:center><span style=background-color:light-gray><code>high pass</code></span></th><th style=text-align:center><span style=background-color:light-gray><code>motion</code></span></th><th style=text-align:center><span style=background-color:light-gray><code>wm csf</code></span></th><th style=text-align:center><span style=background-color:light-gray><code>global</code></span></th><th style=text-align:center><span style=background-color:light-gray><code>compcor</code></span></th><th style=text-align:center><span style=background-color:light-gray><code>ica aroma</code></span></th><th style=text-align:center><span style=background-color:light-gray><code>scrub</code></span></th></tr></thead><tbody><tr class=odd><td style=text-align:left><span style=background-color:light-gray><code>Params2</code></span></td><td style=text-align:center>x</td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=even><td style=text-align:left><span style=background-color:light-gray><code>Params6</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=odd><td style=text-align:left><span style=background-color:light-gray><code>Params9</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td></tr><tr class=even><td style=text-align:left><span style=background-color:light-gray><code>Params9scrub</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=odd><td style=text-align:left><span style=background-color:light-gray><code>Params24</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=even><td style=text-align:left><span style=background-color:light-gray><code>Params36</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td></tr><tr class=odd><td style=text-align:left><span style=background-color:light-gray><code>Params36scrub</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=even><td style=text-align:left><span style=background-color:light-gray><code>AnatCompCor</code></span></td><td style=text-align:center>x</td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>anat</code></span></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=odd><td style=text-align:left><span style=background-color:light-gray><code>TempCompCor</code></span></td><td style=text-align:center>x</td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>temp</code></span></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr class=even><td style=text-align:left><span style=background-color:light-gray><code>ICAAROMA</code></span></td><td style=text-align:center>x</td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center></td></tr><tr class=odd><td style=text-align:left><span style=background-color:light-gray><code>AROMAGSR</code></span></td><td style=text-align:center>x</td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>full</code></span></td><td style=text-align:center></td></tr><tr class=even><td style=text-align:left><span style=background-color:light-gray><code>AggrICAAROMA</code></span></td><td style=text-align:center>x</td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td><td style=text-align:center><span style=background-color:light-gray><code>basic</code></span></td><td style=text-align:center></td></tr></tbody></table><p>The future direction is to integrate <span style=background-color:light-gray><code>load confounds</code></span> in <span style=background-color:light-gray><code>nilearn</code></span> for better reach to a wider range of users who may benifit from the package. To facilitate the nilearn inegration, we will add <span style=background-color:light-gray><code>sample mask</code></span> to support volume-sensoring based scrubbing and improve the <span style=background-color:light-gray><code>nilearn</code></span> <span style=background-color:light-gray><code>NiftiMasker</code></span> related feature.</p><h1 class=unnumbered id=references>References</h1><div id=refs class="references hanging-indent" role=doc-bibliography><div id=ref-compcor:2007><p>Behzadi, Yashar, Khaled Restom, Joy Liau, and Thomas T. Liu. 2007. “A Component Based Noise Correction Method (Compcor) for Bold and Perfusion Based fMRI.” <em>NeuroImage</em> 37 (1): 90–101. <a href=https://doi.org/10.1016/j.neuroimage.2007.04.042>https://doi.org/10.1016/j.neuroimage.2007.04.042</a>.</p></div><div id=ref-ciric:2017><p>Ciric, Rastko, Daniel H. Wolf, Jonathan D. Power, David R. Roalf, Graham L. Baum, Kosha Ruparel, Russell T. Shinohara, et al. 2017. “Benchmarking of Participant-Level Confound Regression Strategies for the Control of Motion Artifact in Studies of Functional Connectivity.” <em>Neuroimage</em> 154: 174–87.</p></div><div id=ref-fmriprep:2019><p>Esteban, Oscar, Christopher J Markiewicz, Ross W Blair, Craig A Moodie, A Ilkay Isik, Asier Erramuzpe, James D Kent, et al. 2019. “FMRIPrep: A Robust Preprocessing Pipeline for Functional Mri.” <em>Nature Methods</em> 16 (1): 111–16.</p></div><div id=ref-jenkinson:2002><p>Jenkinson, Mark, Peter Bannister, Michael Brady, and Stephen Smith. 2002. “Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images.” <em>NeuroImage</em> 17 (2): 825–41. <a href=https://doi.org/10.1006/nimg.2002.1132>https://doi.org/10.1006/nimg.2002.1132</a>.</p></div><div id=ref-power:2012><p>Power, Jonathan D., Kelly A. Barnes, Abraham Z. Snyder, Bradley L. Schlaggar, and Steven E. Petersen. 2012. “Spurious but Systematic Correlations in Functional Connectivity Mri Networks Arise from Subject Motion.” <em>NeuroImage</em> 59 (3): 2142–54. <a href=https://doi.org/10.1016/j.neuroimage.2011.10.018>https://doi.org/10.1016/j.neuroimage.2011.10.018</a>.</p></div><div id=ref-icaaroma:2015><p>Pruim, Raimon H. R., Maarten Mennes, Daan van Rooij, Alberto Llera, Jan K. Buitelaar, and Christian F. Beckmann. 2015. “ICA-Aroma: A Robust Ica-Based Strategy for Removing Motion Artifacts from fMRI Data.” <em>NeuroImage</em> 112: 267–77. <a href=https://doi.org/10.1016/j.neuroimage.2015.02.064>https://doi.org/10.1016/j.neuroimage.2015.02.064</a>.</p></div></div></body></html></main></div></div><style>footer{margin-top:2rem;padding:1rem;background-color:#424344!important}footer .logo{font-size:1.3rem}</style><footer><div class=container><p class="m-0 text-center text-white"><a class=text-white href=https://github.com/brainhack-proceedings target=_blank><span class="logo logo-brainhack">BRAINHACK</span><span class="logo logo-proceedings">Proceedings</span></a></a></p><p class="m-0 text-center text-white">Copyright &copy; BrainHack 2021</p></div></footer></body></html>